# Performance & Benchmarks

## Benchmark Commands
```bash
python benchmark.py --quick          # All benchmarks
python benchmark.py --training       # RMSNorm + AdamW
python benchmark.py --activations    # GELU + SiLU
python benchmark.py --sdpa           # Scaled Dot Product Attention
```

## Training Ops (v0.1.8)

| Operation | Size | Metal vs Torch | Status |
|-----------|------|----------------|--------|
| RMSNorm | 4096Ã—4096 | **2.5x faster** | ğŸ’š |
| AdamW | 16M params | **2.9x faster** | ğŸ’š |
| SiLU | 256Ã—1024 | **4x faster** | ğŸ’š |
| GELU | 1024Ã—4096 | ~1x (parity) | âšª |
| SDPA | N=256 | 14x slower | ğŸ”´ |
| CrossEntropy | Lite | **18x faster** (fp32) | ğŸ’š |
| KL Div | Lite | **5x faster** (fp32) | ğŸ’š |
| SwiGLU MLP | Lite | ~1.03x (FP32, crash fix verif) | ğŸ”µ |

**Note**: SDPA is slower than PyTorch's native implementation (which uses Apple's MPS optimizations). Use `enable_metal_sdpa()` only if you need custom backward pass behavior.

## Linear Algebra Ops

| Operation | Size | GPU vs CPU |
|-----------|------|------------|
| **Gemma-7B MLP SVD** | 3072Ã—24576 | **25x faster** |
| **Cholesky batched** | 500Ã—16Ã—16 | **33x faster** |
| **QR batched** | 1000Ã—16Ã—16 | **14x faster** |
| **Llama-3-8B SVD** | 4096Ã—14336 | **5.9x faster** |

## When to Use Metal

| âœ… Use Metal | âŒ Use CPU/Native |
|--------------|-------------------|
| Batched QR/Cholesky/SVD | Single small matrices |
| Large LLM weight matrices | SDPA (use native F.sdpa) |
| RMSNorm in training | Sequential operations |
| AdamW optimizer step | Small batch operations |
