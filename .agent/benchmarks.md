# Performance & Benchmarks

## Benchmark Commands
```bash
python benchmark.py --quick          # All benchmarks
python benchmark.py --training       # RMSNorm + AdamW
python benchmark.py --activations    # GELU + SiLU
python benchmark.py --sdpa           # Scaled Dot Product Attention
```

## Training Ops (v0.1.6)

| Operation | Size | Metal vs Torch | Status |
|-----------|------|----------------|--------|
| RMSNorm | 4096√ó4096 | **2.5x faster** | üíö |
| AdamW | 16M params | **2.9x faster** | üíö |
| SiLU | 256√ó1024 | **4x faster** | üíö |
| GELU | 1024√ó4096 | ~1x (parity) | ‚ö™ |
| SDPA | N=256 | 14x slower | üî¥ |

**Note**: SDPA is slower than PyTorch's native implementation (which uses Apple's MPS optimizations). Use `enable_metal_sdpa()` only if you need custom backward pass behavior.

## Linear Algebra Ops

| Operation | Size | GPU vs CPU |
|-----------|------|------------|
| **Gemma-7B MLP SVD** | 3072√ó24576 | **25x faster** |
| **Cholesky batched** | 500√ó16√ó16 | **33x faster** |
| **QR batched** | 1000√ó16√ó16 | **14x faster** |
| **Llama-3-8B SVD** | 4096√ó14336 | **5.9x faster** |

## When to Use Metal

| ‚úÖ Use Metal | ‚ùå Use CPU/Native |
|--------------|-------------------|
| Batched QR/Cholesky/SVD | Single small matrices |
| Large LLM weight matrices | SDPA (use native F.sdpa) |
| RMSNorm in training | Sequential operations |
| AdamW optimizer step | Small batch operations |
