# Metalops Benchmark Results

*Generated: 2026-01-13 15:26:26*

**Legend:** ðŸ’š GPU wins big (>3x) | ðŸŸ¢ GPU wins | ðŸ”µ Close | âšª CPU wins | ðŸŸ  CPU wins big (>3x)

## QR Batched (metalcore) â­ GPU WINS

*Batched QR - GPU processes all matrices in parallel, single dispatch*

| Shape | Config | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 50Ã—8Ã—8 | Tiny 8x8 | 709.7Âµs | 2.3ms | 0.31x | ðŸŸ¢ | âœ“ 6e-07 |
| 100Ã—8Ã—8 | Batch 100 tiny | 701.3Âµs | 4.6ms | 0.15x | ðŸ’š | âœ“ 4e-07 |
| 500Ã—8Ã—8 | Batch 500 tiny | 714.9Âµs | 22.4ms | 0.03x | ðŸ’š | âœ“ 6e-07 |
| 50Ã—16Ã—16 | ML mini-batch 16 | 601.5Âµs | 2.3ms | 0.26x | ðŸ’š | âœ“ 1e-06 |
| 100Ã—16Ã—16 | Batch 100 16x16 | 558.9Âµs | 4.7ms | 0.12x | ðŸ’š | âœ“ 1e-06 |
| 200Ã—16Ã—16 | Batch 200 16x16 | 610.8Âµs | 9.4ms | 0.07x | ðŸ’š | âœ“ 6e-07 |
| 500Ã—16Ã—16 | Batch 500 16x16 | 594.0Âµs | 23.6ms | 0.03x | ðŸ’š | âœ“ 1e-06 |
| 1000Ã—16Ã—16 | Batch 1000 16x16 | 722.1Âµs | 47.0ms | 0.02x | ðŸ’š | âœ“ 9e-07 |
| 50Ã—32Ã—32 | ML mini-batch 32 | 638.8Âµs | 2.7ms | 0.24x | ðŸ’š | âœ“ 1e-06 |
| 100Ã—32Ã—32 | Batch 100 32x32 | 581.3Âµs | 5.4ms | 0.11x | ðŸ’š | âœ“ 1e-06 |
| 200Ã—32Ã—32 | Batch 200 32x32 | 531.5Âµs | 10.8ms | 0.05x | ðŸ’š | âœ“ 1e-06 |
| 500Ã—32Ã—32 | Batch 500 32x32 | 807.6Âµs | 27.0ms | 0.03x | ðŸ’š | âœ“ 1e-06 |
| 50Ã—48Ã—48 | Batch 50 48x48 | 32.2ms | 3.1ms | 10.30x | ðŸŸ  | âœ“ 1e-06 |
| 100Ã—48Ã—48 | Batch 100 48x48 | 63.8ms | 6.5ms | 9.83x | ðŸŸ  | âœ“ 1e-06 |
| 100Ã—64Ã—32 | Tall batch | 62.9ms | 5.8ms | 10.80x | ðŸŸ  | âœ“ 1e-06 |
| 100Ã—32Ã—64 | Wide batch | 899.4Âµs | 5.6ms | 0.16x | ðŸ’š | âœ— nan |
| 200Ã—64Ã—32 | Large tall batch | 125.3ms | 12.2ms | 10.30x | ðŸŸ  | âœ“ 1e-06 |

## Cholesky (metalcore) â­ GPU WINS

*Batched Cholesky decomposition with MAGMA-style shared memory*

| Shape | Config | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 100Ã—16Ã—16 | Tiny batched | 399.0Âµs | 4.4ms | 0.09x | ðŸ’š | âœ“ 6e-06 |
| 500Ã—16Ã—16 | Large batch tiny | 573.5Âµs | 21.4ms | 0.03x | ðŸ’š | âœ“ 4e-06 |
| 100Ã—32Ã—32 | Small batched | 588.0Âµs | 4.3ms | 0.14x | ðŸ’š | âœ“ 8e-06 |
| 200Ã—48Ã—48 | Medium batched | 908.2Âµs | 8.7ms | 0.10x | ðŸ’š | âœ“ 2e-05 |
| 100Ã—64Ã—64 | Larger batched | 1.3ms | 4.5ms | 0.29x | ðŸ’š | âœ“ 2e-05 |

## RMSNorm (metalcore) â­ GPU WINS

*Fused RMSNorm kernel vs torch.nn.RMSNorm*

| Shape | Config | Metal | CPU | Ratio | Status |
|---|---|---|---|---|---|
| 32x4096 | Fwd+Bwd fp32 | 465.1Âµs | 506.7Âµs | 0.92x | ðŸ”µ |
| 1x4096 | Fwd+Bwd fp32 | 424.3Âµs | 449.5Âµs | 0.94x | ðŸ”µ |
| 1024x1024 | Fwd+Bwd fp32 | 792.6Âµs | 909.2Âµs | 0.87x | ðŸ”µ |
| 4096x4096 | Fwd+Bwd fp32 | 5.9ms | 8.6ms | 0.69x | ðŸŸ¢ |

## AdamW (metalcore) â­ GPU WINS

*Fused AdamW optimizer step vs torch.optim.AdamW*

| Params | Size | Metal | CPU | Ratio | Status |
|---|---|---|---|---|---|
| 1M Params | N=1048576 fp32 | 330.9Âµs | 525.3Âµs | 0.63x | ðŸŸ¢ |
| 10M Params | N=10485760 fp32 | 1.1ms | 2.9ms | 0.38x | ðŸŸ¢ |
| 16M Params | N=16777216 fp32 | 1.6ms | 4.5ms | 0.36x | ðŸŸ¢ |

## Pipeline Operations â­ GPU WINS (No Transfer)

*Chained operations where data stays on GPU - avoids costly memory transfers*

| Pipeline | Shape | GPU | Comparison | Ratio | Status |
|---|---|---|---|---|---|
| QR -> QR -> QR | 200Ã—32Ã—32 | 904.2Âµs | 32.0ms | 0.03x | ðŸ’š |
| SVD -> truncate (PCA) | 50Ã—128Ã—64 | 11.1ms | 16.0ms | 0.69x | ðŸŸ¢ |
| QR -> matmul (ML) | 1000Ã—16Ã—16 | 1.3ms | 51.5ms | 0.03x | ðŸ’š |
| Fast+Slow+Fast (GPU all) | 200Ã—32Ã—32 | 2.0ms | 1.2ms | 1.65x vs hybrid | ðŸŸ  |
| Fast+Slow+Fast (vs CPU) | 200Ã—32Ã—32 | 2.0ms | 12.0ms | 0.17x vs CPU | ðŸ’š |

## LLM: Llama

*SVD performance on Llama weight matrix sizes*

| Shape | Layer | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 4096Ã—4096 | Attention (7B) | 2.85s | 5.00s | 0.57x | ðŸŸ¢ | âœ“ 3e-05 |
| 4096Ã—11008 | MLP up (7B) | 2.92s | 10.00s | 0.29x | ðŸ’š | âœ“ 3e-05 |
| 8192Ã—8192 | Attention (70B) | 21.90s | 36.95s | 0.59x | ðŸŸ¢ | ~ 1e-04 |

## LLM: Mistral

*SVD performance on Mistral weight matrix sizes*

| Shape | Layer | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 4096Ã—4096 | Attention | 2.89s | 5.09s | 0.57x | ðŸŸ¢ | âœ“ 3e-05 |
| 4096Ã—14336 | MLP up | 2.99s | 11.93s | 0.25x | ðŸ’š | âœ“ 3e-05 |

## LLM: Qwen

*SVD performance on Qwen weight matrix sizes*

| Shape | Layer | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 4096Ã—4096 | Attention | 2.89s | 5.10s | 0.57x | ðŸŸ¢ | âœ“ 3e-05 |
| 4096Ã—11008 | MLP up | 2.99s | 10.17s | 0.29x | ðŸ’š | âœ“ 3e-05 |

## LLM: Gemma

*SVD performance on Gemma weight matrix sizes*

| Shape | Layer | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 3072Ã—3072 | Attention | 927.5ms | 1.60s | 0.58x | ðŸŸ¢ | âœ“ 2e-05 |
| 3072Ã—24576 | MLP up | 997.8ms | 9.25s | 0.11x | ðŸ’š | âœ“ 3e-05 |

## LLM: Phi

*SVD performance on Phi weight matrix sizes*

| Shape | Layer | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 3072Ã—3072 | Attention | 919.7ms | 1.61s | 0.57x | ðŸŸ¢ | âœ“ 2e-05 |
| 3072Ã—8192 | MLP up | 937.5ms | 3.74s | 0.25x | ðŸ’š | âœ“ 2e-05 |

## Activations (metalcore)

*GELU/SiLU activations with float4 vectorization*

| Op | Shape | Metal | Torch | Ratio | Status |
|---|---|---|---|---|---|
| GELU Small (256x1024) | 256x1024 fp32 | 182.0Âµs | 206.5Âµs | 0.88x | ðŸ”µ |
| SiLU Small (256x1024) | 256x1024 fp32 | 187.2Âµs | 203.6Âµs | 0.92x | ðŸ”µ |
| GELU Medium (1024x4096) | 1024x4096 fp32 | 286.6Âµs | 272.5Âµs | 1.05x | ðŸ”µ |
| SiLU Medium (1024x4096) | 1024x4096 fp32 | 261.4Âµs | 275.3Âµs | 0.95x | ðŸ”µ |
| GELU Large (4096x4096) | 4096x4096 fp32 | 643.6Âµs | 632.8Âµs | 1.02x | ðŸ”µ |
| SiLU Large (4096x4096) | 4096x4096 fp32 | 631.2Âµs | 650.1Âµs | 0.97x | ðŸ”µ |

## Eigendecomposition (metaleig)

*Symmetric eigenvalue decomposition*

| Shape | Config | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 32Ã—32 | Tiny | 837.3Âµs | 49.2Âµs | 17.01x | ðŸŸ  | âœ“ 3e-06 |
| 64Ã—64 | Small | 776.8Âµs | 176.9Âµs | 4.39x | ðŸŸ  | âœ“ 2e-06 |
| 128Ã—128 | Medium | 1.2ms | 624.8Âµs | 1.96x | âšª | âœ“ 4e-06 |
| 256Ã—256 | Large | 4.0ms | 2.7ms | 1.45x | âšª | âœ“ 7e-06 |
| 512Ã—512 | Very large | 13.8ms | 12.1ms | 1.14x | ðŸ”µ | âœ“ 1e-05 |
| 1024Ã—1024 | Huge | 67.3ms | 64.9ms | 1.04x | ðŸ”µ | âœ“ 3e-05 |
| 100Ã—32Ã—32 | Batch 100 tiny | 5.7ms | 4.7ms | 1.22x | ðŸ”µ | âœ— 2e+00 |
| 50Ã—64Ã—64 | Batch 50 small | 4.5ms | 8.9ms | 0.51x | ðŸŸ¢ | ~ 2e-03 |
| 100Ã—64Ã—64 | Batch 100 small | 7.9ms | 17.8ms | 0.45x | ðŸŸ¢ | ~ 2e-04 |
| 200Ã—64Ã—64 | Batch 200 small | 14.4ms | 35.4ms | 0.41x | ðŸŸ¢ | ~ 3e-04 |
| 20Ã—128Ã—128 | Batch 20 medium | 5.1ms | 11.3ms | 0.45x | ðŸŸ¢ | ~ 2e-03 |
| 50Ã—128Ã—128 | Batch 50 medium | 9.2ms | 28.3ms | 0.33x | ðŸŸ¢ | ~ 1e-03 |
| 10Ã—256Ã—256 | Batch 10 large | 28.0ms | 26.2ms | 1.07x | ðŸ”µ | âœ“ 7e-06 |

## SVD (metalsvd)

*Singular Value Decomposition using Jacobi algorithm on GPU*

| Shape | Config | Metal | CPU | Ratio | Status | Recon Error |
|---|---|---|---|---|---|---|
| 32Ã—32 | Tiny | 945.1Âµs | 77.2Âµs | 12.24x | ðŸŸ  | âœ“ 2e-06 |
| 64Ã—64 | Small square | 1.0ms | 230.1Âµs | 4.56x | ðŸŸ  | âœ“ 4e-06 |
| 128Ã—128 | Medium square | 1.6ms | 781.4Âµs | 2.07x | âšª | âœ“ 5e-06 |
| 256Ã—256 | Large square | 5.3ms | 4.0ms | 1.31x | âšª | âœ“ 8e-06 |
| 512Ã—512 | Very large | 14.4ms | 17.5ms | 0.82x | ðŸ”µ | âœ“ 9e-06 |
| 1024Ã—1024 | Huge square | 67.1ms | 92.3ms | 0.73x | ðŸ”µ | âœ“ 1e-05 |
| 2048Ã—2048 | Massive square | 338.1ms | 515.2ms | 0.66x | ðŸŸ¢ | âœ“ 2e-05 |
| 256Ã—128 | Tall 2:1 | 2.3ms | 1.7ms | 1.33x | âšª | âœ“ 5e-06 |
| 512Ã—256 | Tall 2:1 large | 7.4ms | 8.5ms | 0.87x | ðŸ”µ | âœ“ 9e-06 |
| 1024Ã—512 | Tall matrix | 14.6ms | 39.1ms | 0.37x | ðŸŸ¢ | âœ“ 1e-05 |
| 2048Ã—512 | Very tall | 14.3ms | 105.0ms | 0.14x | ðŸ’š | âœ“ 9e-06 |
| 128Ã—256 | Wide 1:2 | 2.3ms | 1.8ms | 1.27x | ðŸ”µ | âœ“ 6e-06 |
| 4096Ã—4096 | Llama-7B attn (4096x4096) | 2.83s | 4.95s | 0.57x | ðŸŸ¢ | ~ 3e-04 |
| 4096Ã—11008 | Llama-2-7B MLP (4096x11008) | 2.89s | 12.18s | 0.24x | ðŸ’š | âœ“ 3e-05 |
| 4096Ã—14336 | Llama-3-8B MLP (4096x14336) | 2.96s | 17.14s | 0.17x | ðŸ’š | âœ“ 3e-05 |
| 8192Ã—8192 | Llama-70B attn (8192x8192) | 22.28s | 37.94s | 0.59x | ðŸŸ¢ | ~ 2e-04 |
| 4096Ã—14336 | Mistral-7B MLP (4096x14336) | 2.94s | 17.14s | 0.17x | ðŸ’š | âœ“ 4e-05 |
| 4096Ã—11008 | Qwen-7B MLP (4096x11008) | 2.94s | 11.73s | 0.25x | ðŸ’š | âœ“ 3e-05 |
| 3072Ã—24576 | Gemma-7B MLP (3072x24576) | 1.00s | 28.22s | 0.04x | ðŸ’š | âœ“ 2e-05 |
| 3072Ã—8192 | Phi-3-mini MLP (3072x8192) | 933.2ms | 5.29s | 0.18x | ðŸ’š | âœ“ 2e-05 |
| 100Ã—32Ã—32 | Batch 100 tiny | 5.5ms | 7.1ms | 0.77x | ðŸ”µ | âœ“ 3e-06 |
| 50Ã—64Ã—64 | Batch 50 small | 5.7ms | 11.7ms | 0.49x | ðŸŸ¢ | âœ“ 5e-06 |
| 100Ã—64Ã—64 | Batch 100 small | 8.6ms | 23.3ms | 0.37x | ðŸŸ¢ | âœ“ 6e-06 |
| 200Ã—64Ã—64 | Batch 200 small | 14.6ms | 46.5ms | 0.31x | ðŸŸ¢ | âœ“ 5e-06 |
| 20Ã—128Ã—128 | Batch 20 medium | 7.4ms | 16.1ms | 0.46x | ðŸŸ¢ | âœ“ 8e-06 |
| 50Ã—128Ã—128 | Batch 50 medium | 10.5ms | 40.2ms | 0.26x | ðŸ’š | âœ“ 9e-06 |
| 10Ã—256Ã—256 | Batch 10 large | 47.0ms | 41.9ms | 1.12x | ðŸ”µ | âœ“ 2e-05 |
| 5Ã—512Ã—512 | Batch 5 huge | 61.3ms | 89.3ms | 0.69x | ðŸŸ¢ | âœ“ 1e-05 |

## QR Single Matrix (metalcore)

*Single matrix QR - CPU typically wins due to sequential dependencies*

| Shape | Config | Metal | CPU | Ratio | Status | Recon | Ortho |
|---|---|---|---|---|---|---|---|
| 16Ã—16 | Tiny | 1.1ms | 40.7Âµs | 27.74x | ðŸŸ  | âœ“ 7e-07 | âœ“ 2e-07 |
| 32Ã—32 | Small 32 | 576.2Âµs | 55.0Âµs | 10.48x | ðŸŸ  | âœ“ 1e-06 | âœ“ 3e-07 |
| 64Ã—64 | Small 64 | 791.4Âµs | 89.9Âµs | 8.80x | ðŸŸ  | âœ“ 1e-06 | âœ“ 6e-07 |
| 128Ã—128 | Medium | 942.3Âµs | 227.1Âµs | 4.15x | ðŸŸ  | âœ“ 4e-06 | âœ“ 6e-07 |
| 256Ã—256 | Large 256 | 1.8ms | 995.6Âµs | 1.77x | âšª | âœ“ 5e-06 | âœ“ 8e-07 |
| 512Ã—512 | Large 512 | 5.3ms | 4.1ms | 1.30x | ðŸ”µ | âœ“ 7e-06 | âœ“ 1e-06 |
| 1024Ã—1024 | Huge 1024 | 26.7ms | 24.5ms | 1.09x | ðŸ”µ | âœ“ 1e-05 | âœ“ 2e-06 |
| 256Ã—64 | Tall 4:1 | 1.5ms | 223.4Âµs | 6.80x | ðŸŸ  | âœ“ 2e-06 | âœ“ 7e-07 |
| 256Ã—128 | Tall 2:1 | 1.2ms | 538.6Âµs | 2.25x | âšª | âœ“ 3e-06 | âœ“ 7e-07 |
| 512Ã—128 | Tall 4:1 large | 1.7ms | 1.0ms | 1.68x | âšª | âœ“ 6e-06 | âœ“ 8e-07 |
| 512Ã—256 | Tall 2:1 large | 3.0ms | 1.8ms | 1.63x | âšª | âœ“ 6e-06 | âœ“ 1e-06 |
| 1000Ã—200 | Tall 5:1 | 3.8ms | 2.5ms | 1.51x | âšª | âœ“ 9e-06 | âœ“ 1e-06 |
| 2000Ã—500 | Huge tall | 15.4ms | 13.7ms | 1.13x | ðŸ”µ | âœ“ 1e-05 | âœ“ 2e-06 |
| 4000Ã—1000 | Massive | 79.7ms | 77.4ms | 1.03x | ðŸ”µ | âœ“ 2e-05 | âœ“ 3e-06 |

## SDPA (metalcore)

*Scaled Dot Product Attention with Flash Attention v2 tiling*

| Config | Shape | Metal | Torch | Ratio | Status | Error |
|---|---|---|---|---|---|---|
| Small (B=2, H=8, N=64, D=64) | B=2, H=8, N=64, D=64 fp32 | 233.0Âµs | 302.0Âµs | 0.77x | ðŸ”µ | âœ“ 7e-07 |
| Small (B=2, H=8, N=64, D=64) (causal) | B=2, H=8, N=64, D=64 fp32 | 671.4Âµs | 286.6Âµs | 2.34x | âšª | âœ“ 3e-06 |
| Medium (B=2, H=8, N=256, D=64) | B=2, H=8, N=256, D=64 fp32 | 929.4Âµs | 394.6Âµs | 2.36x | âšª | âœ— 1e-01 |
| Medium (B=2, H=8, N=256, D=64) (causal) | B=2, H=8, N=256, D=64 fp32 | 3.4ms | 406.7Âµs | 8.42x | ðŸŸ  | âœ“ 3e-06 |
| Large (B=1, H=8, N=512, D=64) | B=1, H=8, N=512, D=64 fp32 | 1.7ms | 454.8Âµs | 3.71x | ðŸŸ  | âœ— 4e-02 |
| Large (B=1, H=8, N=512, D=64) (causal) | B=1, H=8, N=512, D=64 fp32 | 5.8ms | 479.1Âµs | 12.17x | ðŸŸ  | âœ“ 3e-06 |

## Usage Recommendations

| Operation | When to Use Metal | When to Use CPU |
|---|---|---|
| EIGH | Batched symmetric matrices | Single large matrices |
| Pipeline | Keep data on GPU to avoid transfer cost | Single ops on CPU-resident data |
| QR (batched) | Many small matrices (10x speedup!) | Few matrices |
| QR (single) | â€” | Always (sequential dependencies) |
| SVD | Batched small/medium matrices | Single large matrices |

## Linear Solve (metalcore)

*Batched linear system solve using QR + TRSM*

| Shape | Config | Metal | CPU | Ratio | Status | Residual |
|---|---|---|---|---|---|---|
| 100Ã—16Ã—16 | Tiny batched fp32 | 313.3Âµs | 1.3ms | 0.25x | ðŸ’š | âœ“ 4e-05 |
| 500Ã—16Ã—16 | Large batch tiny fp32 | 291.1Âµs | 6.1ms | 0.05x | ðŸ’š | âœ— 1e-02 |
| 100Ã—32Ã—32 | Small batched fp32 | 856.9Âµs | 1.6ms | 0.52x | ðŸŸ¢ | ~ 2e-03 |
| 200Ã—48Ã—48 | Medium batched fp32 | 878.1Âµs | 3.8ms | 0.23x | ðŸ’š | ~ 2e-04 |

## Fused Softmax (metalcore)

*Online softmax algorithm with SIMD reductions*

| Config | Shape | Metal | Torch | Ratio | Status | Error |
|---|---|---|---|---|---|---|
| Small | 32x1024 fp32 | 165.1Âµs | 186.4Âµs | 0.89x | ðŸ”µ | 3.73e-09 |
| Medium | 64x4096 fp32 | 159.8Âµs | 201.8Âµs | 0.79x | ðŸ”µ | 9.31e-10 |
| Large | 128x8192 fp32 | 182.1Âµs | 220.1Âµs | 0.83x | ðŸ”µ | 6.98e-10 |
| Very Large | 256x16384 fp32 | 259.9Âµs | 290.1Âµs | 0.90x | ðŸ”µ | 9.31e-10 |
| Huge | 512x32768 fp32 | 950.9Âµs | 831.6Âµs | 1.14x | ðŸ”µ | 2.33e-10 |
| LLM Vocab | 32x32000 fp32 | 216.9Âµs | 248.9Âµs | 0.87x | ðŸ”µ | 2.33e-10 |
| LLM Vocab Large | 128x128000 fp32 | 937.1Âµs | 824.8Âµs | 1.14x | ðŸ”µ | 1.16e-10 |

## LayerNorm (metalcore)

*Welford's algorithm for fused mean/variance*

| Config | Shape | Metal | Torch | Ratio | Status | Error |
|---|---|---|---|---|---|---|
| Tiny | 32x512 fp32 | 171.5Âµs | 169.6Âµs | 1.01x | ðŸ”µ | 4.77e-07 |
| Small | 64x1024 fp32 | 163.9Âµs | 169.9Âµs | 0.96x | ðŸ”µ | 4.77e-07 |
| Llama-7B | 32x4096 fp32 | 175.7Âµs | 170.2Âµs | 1.03x | ðŸ”µ | 4.77e-07 |
| Llama-13B | 32x5120 fp32 | 173.6Âµs | 170.8Âµs | 1.02x | ðŸ”µ | 4.77e-07 |
| Llama-70B | 16x8192 fp32 | 179.0Âµs | 173.1Âµs | 1.03x | ðŸ”µ | 4.77e-07 |
| Large Batch | 256x4096 fp32 | 188.4Âµs | 194.2Âµs | 0.97x | ðŸ”µ | 9.54e-07 |
| Huge Batch | 1024x4096 fp32 | 242.2Âµs | 268.6Âµs | 0.90x | ðŸ”µ | 4.77e-07 |

## Embedding Bag (metalcore)

*Coalesced reads for embedding lookups and aggregation*

| Config | Shape | Metal | Torch | Ratio | Status |
|---|---|---|---|---|---|
| Small Vocab | 10000x64, B=32 | 227.0Âµs | 1.3ms | 0.17x | ðŸ’š |
| Medium Vocab | 50000x128, B=64 | 232.7Âµs | 1.7ms | 0.14x | ðŸ’š |
| Large Vocab | 100000x256, B=32 | 274.4Âµs | 6.5ms | 0.04x | ðŸ’š |
| LLM Embedding | 32000x4096, B=16 | 348.1Âµs | 20.6ms | 0.02x | ðŸ’š |
| Huge Vocab | 250000x512, B=16 | 246.1Âµs | 19.8ms | 0.01x | ðŸ’š |

## Scatter/Gather (metalcore)

*Atomic scatter_add and vectorized gather operations*

| Op | Shape | Metal | Torch | Ratio | Status |
|---|---|---|---|---|---|
| Gather Small | src=10000, idx=1000 | 204.1Âµs | 223.9Âµs | 0.91x | ðŸ”µ |
| Gather Medium | src=100000, idx=10000 | 210.6Âµs | 229.0Âµs | 0.92x | ðŸ”µ |
| Gather Large | src=1000000, idx=100000 | 219.1Âµs | 238.9Âµs | 0.92x | ðŸ”µ |
| Gather Huge | src=10000000, idx=1000000 | 353.6Âµs | 357.2Âµs | 0.99x | ðŸ”µ |
| ScatterAdd Small | dst=10000, idx=1000 | 213.1Âµs | 228.0Âµs | 0.93x | ðŸ”µ |
| ScatterAdd Medium | dst=100000, idx=10000 | 185.6Âµs | 217.0Âµs | 0.86x | ðŸ”µ |
| ScatterAdd Large | dst=1000000, idx=100000 | 253.5Âµs | 269.4Âµs | 0.94x | ðŸ”µ |
| ScatterAdd Huge | dst=10000000, idx=1000000 | 953.6Âµs | 995.0Âµs | 0.96x | ðŸ”µ |

## LoRA Training Ops (metalcore)

*Fused operations for LoRA fine-tuning: cross-entropy, KL divergence, SwiGLU, LoRA linear*

| Op | Config | Metal | Torch | Ratio | Status |
|---|---|---|---|---|---|
| CrossEntropy | 128x32000 Llama vocab | 244.0Âµs | 431.0Âµs | 0.57x | ðŸŸ¢ |
| CrossEntropy | 256x32000 Large batch | 310.6Âµs | 665.9Âµs | 0.47x | ðŸŸ¢ |
| CrossEntropy | 512x128256 Llama-3 vocab | 1.6ms | 3.2ms | 0.51x | ðŸŸ¢ |
| KL Divergence | 128x32000 Full vocab | 3.1ms | 650.4Âµs | 4.81x | ðŸŸ  |
| KL Divergence | 256x32000 Large batch | 4.5ms | 1.1ms | 3.98x | ðŸŸ  |
| KL-TopK | 128x32000 k=100 (100% saved) | 269.0Âµs | 340.5Âµs | 0.79x | ðŸ”µ |
| KL-TopK | 256x32000 k=50 (100% saved) | 252.3Âµs | 304.0Âµs | 0.83x | ðŸ”µ |
| SwiGLU | 128x11008 Llama-7B hidden | 230.2Âµs | 320.5Âµs | 0.72x | ðŸ”µ |
| SwiGLU | 256x14336 Llama-3 hidden | 363.8Âµs | 387.8Âµs | 0.94x | ðŸ”µ |
| LoRA Linear | 128x4096â†’4096 r=16 Llama attn r=16 | 881.9Âµs | 885.2Âµs | 1.00x | ðŸ”µ |
| LoRA Linear | 128x4096â†’11008 r=8 Llama MLP r=8 | 1.6ms | 1.6ms | 1.01x | ðŸ”µ |

## Fused Backward Operations (Phase 3)

*Benchmarks run in Lite mode (fewer iterations)*

| Op | Config | Metal | Torch | Ratio | Status |
|---|---|---|---|---|---|
| FusedAtt Bwd | Lite Attn fp32 | 4.6ms | 272.2ms | 0.02x | ðŸ’š |
| FusedAtt Bwd | Lite Attn fp16 | 6.7ms | 266.2ms | 0.03x | ðŸ’š |
| FusedAtt Bwd | Lite Attn bf16 | 4.3ms | 245.6ms | 0.02x | ðŸ’š |
| FusedMLP Bwd | Lite MLP fp32 | 38.6ms | 228.2ms | 0.17x | ðŸ’š |
| FusedMLP Bwd | Lite MLP fp16 | 96.7ms | 267.4ms | 0.36x | ðŸŸ¢ |
| FusedMLP Bwd | Lite MLP bf16 | 39.8ms | 210.7ms | 0.19x | ðŸ’š |

## Fused Attention Backward (metalcore)

*Fused Bwd: SDPA Grads -> RoPE Bwd -> QKV Bwd -> RMSNorm Bwd*

| Op | Config | Metal | Torch | Ratio | Status |
|---|---|---|---|---|---|
| FusedAtt Bwd | 32x128 Llama-7B Attn fp32 | 77.6ms | 170.4ms | 0.46x | ðŸŸ¢ |
| FusedAtt Bwd | 8x128 Large Head Count fp32 | 74.3ms | 159.9ms | 0.46x | ðŸŸ¢ |
